print("Import library")
from flask import Flask, request, render_template, jsonify
from werkzeug.utils import secure_filename
import os
import whisper
import torch
# from TTS.api import TTS

# Import shared models first to initialize them
print("Loading shared models...")
import shared_models

# Import RAG inference modules (will use shared models)
from rag_inference_gdm import qa_chain as qa_chain_gdm, compression_retriever as compression_retriever_gdm, related_question_chain as related_question_chain_gdm
from rag_inference_ckd import qa_chain as qa_chain_ckd, compression_retriever as compression_retriever_ckd, related_question_chain as related_question_chain_ckd
from rag_inference_ppd import qa_chain as qa_chain_ppd, compression_retriever as compression_retriever_ppd, related_question_chain as related_question_chain_ppd

from opencc import OpenCC
import uuid
import edge_tts
import base64
import random
import time

print("Finish import")
myuuid = uuid.uuid4()
app = Flask(__name__)
UPLOAD_FOLDER = "temp"
AUDIO_CLONE = "static"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
#init model here
print("init model")
asr_model = whisper.load_model("small")
# device = "cuda" if torch.cuda.is_available() else "cpu"
# tts = TTS(model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST").to(device)
cc = OpenCC('s2twp')
print("Finish init model")
def llm_inference_gdm(user_query):
    """GDM (å¦Šå¨ æœŸç³–å°¿ç—…) inference"""
    question = user_query
    answer = qa_chain_gdm.invoke(question)
    return answer

def llm_inference_ckd(user_query):
    """CKD (æ…¢æ€§è…è‡Ÿç—…) inference"""
    question = user_query
    answer = qa_chain_ckd.invoke(question)
    return answer

def llm_inference_ppd(user_query):
    """PPD (ç”¢å¾Œæ†‚é¬±ç—‡) inference"""
    question = user_query
    answer = qa_chain_ppd.invoke(question)
    return answer

def llm_inference(user_query, model_type="gdm"):
    """é€šç”¨inferenceå‡½æ•¸ï¼Œæ ¹æ“šmodel_typeé¸æ“‡å°æ‡‰çš„æ¨¡å‹"""
    if model_type == "ckd":
        return llm_inference_ckd(user_query)
    elif model_type == "ppd":
        return llm_inference_ppd(user_query)
    else:  # é è¨­ä½¿ç”¨ gdm
        return llm_inference_gdm(user_query)

def load_questions(model_type: str = "gdm"):
    """Load questions file based on model_type/role.

    Supported model_type values: 'gdm', 'ckd', 'ppd'. Defaults to 'gdm'.
    Falls back to a small set of default questions when file is missing.
    """
    mapping = {
        "gdm": "gdm_questions.txt",
        "ckd": "ckd_questions.txt",
        "ppd": "ppd_questions.txt",
    }

    filename = mapping.get(model_type, mapping["gdm"])  # default to gdm
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        return questions
    except FileNotFoundError:
        print(f"âš ï¸  {filename} not found, using default questions")
        return [
            "é€™å€‹ç–¾ç—…çš„ç—‡ç‹€æœ‰å“ªäº›ï¼Ÿ",
            "éœ€è¦æ³¨æ„ä»€éº¼é£²é£Ÿç¦å¿Œï¼Ÿ",
            "è—¥ç‰©çš„å‰¯ä½œç”¨æ˜¯ä»€éº¼ï¼Ÿ",
        ]

@app.route("/")
def index():
    """Trang chá»n chuyÃªn khoa"""
    return render_template("index.html")

@app.route("/chat")
def chat():
    """Trang chat vá»›i chatbot - HTML template version"""
    # Allow selecting model_type via query param, e.g. /chat?model_type=ckd
    model_type = request.args.get('model_type', 'gdm')
    questions = load_questions(model_type)
    random_question = random.choice(questions) if questions else ""
    return render_template("chat.html", questions=questions, random_question=random_question)

@app.route("/api/chat", methods=["GET"])
def api_chat():
    """API endpoint for React frontend to get initial data"""
    print("ğŸ“¥ GET /api/chat - Fetching questions...")
    
    try:
        # allow the frontend to request questions for a specific model_type
        model_type = request.args.get('model_type', 'gdm')
        questions = load_questions(model_type)
        random_question = random.choice(questions) if questions else ""
        
        response = {
            "questions": questions,
            "random_question": random_question,
            "success": True
        }

        print(f"âœ… Returning {len(questions)} questions for model_type={model_type}")
        return jsonify(response)
    except Exception as e:
        print(f"âŒ Error in /api/chat: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "questions": [],
            "random_question": ""
        }), 500

@app.route("/upload", methods=["POST"])
def upload_audio():
    print("ğŸ“¥ POST /upload - Audio upload")
    
    if "audio" not in request.files:
        print("âŒ No audio file in request")
        return "æ²’æœ‰éŸ³è¨Šæª”æ¡ˆ", 400

    file = request.files["audio"]
    
    if file.filename == "":
        print("âŒ Empty filename")
        return "æª”æ¡ˆåç¨±ç‚ºç©º", 400
    
    try:
        filename = secure_filename(file.filename)
        filepath = os.path.join(UPLOAD_FOLDER, filename)
        file.save(filepath)
        print(f"âœ… Audio saved: {filepath}")

        print("ğŸ¤ Transcribing with Whisper...")
        result = asr_model.transcribe(filepath, language="zh")
        
        # Báº¡n cÃ³ thá»ƒ gá»i ASR Ä‘á»ƒ chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh vÄƒn báº£n á»Ÿ Ä‘Ã¢y
        # vÃ­ dá»¥: result = my_asr(filepath)
        answer = cc.convert(result['text'])
        
        print(f"âœ… Transcription: {answer}")
        return answer
    except Exception as e:
        print(f"âŒ Error in upload: {str(e)}")
        return f"éŒ¯èª¤: {str(e)}", 500




@app.get("/ping")
async def ping():
    return {"status": "healthy"}

@app.route("/ask", methods=["POST"])
async def ask():
    print("ğŸ“¥ POST /ask - Processing question")
    
    try:
        question = request.form.get("question")
        role = request.form.get('role', 'unknown')
        gender = request.form.get('gender', 'female')
        model_type = request.form.get('model_type', 'gdm')  # é è¨­ä½¿ç”¨ gdm
        responseWithAudio = request.form.get("responseWithAudio", False)
        
        print(f"   Question: {question}")
        print(f"   Role: {role}")
        print(f"   Model: {model_type}")
        print(f"   Audio: {responseWithAudio}")
        
        if not question:
            print("âŒ No question provided")
            return "è«‹è¼¸å…¥å•é¡Œ", 400

        start_time = time.time()
        print(f"ğŸ”„ Running LLM inference with {model_type} model...")
        answer = llm_inference(question, model_type)
        end_time = time.time() - start_time
        print(f"âœ… LLM: {end_time:.2f}s")
        
        # Remove <think>...</think> tags server-side before any further processing
        import re
        answer = re.sub(r'<think>[\s\S]*?</think>', '', answer)
        
        start_time = time.time()
        answer = cc.convert(answer)
        end_time = time.time() - start_time
        print(f"âœ… Convert: {end_time:.2f}s")
        
        start_time = time.time()
        if responseWithAudio == "true":
            print("ğŸµ Generating audio...")
            # select voice by gender (male -> Yunyang, female -> Xiaoxiao)
            if str(gender).lower() == 'male':
                voices = "zh-CN-YunyangNeural"
            else:
                voices = "zh-CN-XiaoxiaoNeural"
            myuuid = uuid.uuid4()
            audio_name = str(myuuid) + '.mp3'
            filepath = os.path.join(UPLOAD_FOLDER, audio_name)
            tts = edge_tts.Communicate(
                    text=answer,
                    voice=voices)
            await tts.save(filepath)
            # return jsonify({"answer": answer, "audio": audio, "filepath": filepath})
            with open(filepath, 'rb') as audio_file:
                audio_data = audio_file.read()
            audio_base64 = base64.b64encode(audio_data).decode('utf-8')
            end_time = time.time() - start_time
            print(f"âœ… Audio: {end_time:.2f}s")
            return jsonify({"answer": answer, "audio_base64": audio_base64})
        else:
            print("âœ… Returning text answer")
            return jsonify({"answer": answer})
            
    except Exception as e:
        print(f"âŒ Error in ask: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"answer": f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=80, debug=True)
